{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Amazon Augmented AI (Amazon A2I) integration with Amazon SageMaker Hosted Endpoint for Audio Classification and Model Retraining"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Architecture \n",
    "\n",
    "<img src=\"./images/part2.png\" alt=\"architecture\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. A2I Setup \n",
    "\n",
    "a. [Introduction](#Introduction)\n",
    "\n",
    "b. [Setup](#Setup)\n",
    "\n",
    "c. [Create Control Plane Resources](#Create-Control-Plane-Resources)\n",
    "\n",
    "    \n",
    "### 6. Setup workforce and Labeling Manually    \n",
    "a. [Starting Human Loops](#Starting-Human-Loops)\n",
    "\n",
    "b. [Configure a2i status change to SQS](#sqs_a2i)\n",
    "\n",
    "c. [Wait For Workers to Complete Task](#Wait-For-Workers-to-Complete-Task)\n",
    "\n",
    "d. [Check Status of Human Loop](#Check-Status-of-Human-Loop)\n",
    "\n",
    "e. [View Task Results](#View-Task-Results)\n",
    "   \n",
    "### 7. Retrain and Redeploy    \n",
    "[Incremental training with SageMaker](#Incremental-training-with-SageMaker)\n",
    "\n",
    "### 8. Configure Lambda and Api gateway\n",
    "[Create Lambda Function triggering a2i process](#lambda)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Introduction\n",
    "\n",
    "Amazon Augmented AI (Amazon A2I) makes it easy to build the workflows required for human review of ML predictions. Amazon A2I brings human review to all developers, removing the undifferentiated heavy lifting associated with building human review systems or managing large numbers of human reviewers. \n",
    "\n",
    "You can create your own workflows for ML models built on Amazon SageMaker or any other tools. Using Amazon A2I, you can allow human reviewers to step in when a model is unable to make a high confidence prediction or to audit its predictions on an on-going basis. \n",
    "\n",
    "Learn more here: https://aws.amazon.com/augmented-ai/\n",
    "\n",
    "In this tutorial, we will show how you can use **Amazon A2I with an Amazon SageMaker Hosted Endpoint.** We will be using an exisiting audio classification model in this notebook. We will also demonstrate how to manipulate the A2I output to perform incremental training to improve the model accuracy with the newly labeled data using A2I.\n",
    "\n",
    "For more in depth instructions, visit https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-getting-started.html"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To incorporate Amazon A2I into your human review workflows, you need three resources:\n",
    "\n",
    "* A **worker task template** to create a worker UI. The worker UI displays your input data, such as documents or images, and instructions to workers. It also provides interactive tools that the worker uses to complete your tasks. For more information, see https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-instructions-overview.html\n",
    "\n",
    "* A **human review workflow**, also referred to as a flow definition. You use the flow definition to configure your human workforce and provide information about how to accomplish the human review task. You can create a flow definition in the Amazon Augmented AI console or with Amazon A2I APIs. To learn more about both of these options, see https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html\n",
    "\n",
    "* A **human loop** to start your human review workflow. When you use one of the built-in task types, the corresponding AWS service creates and starts a human loop on your behalf when the conditions specified in your flow definition are met or for each object if no conditions were specified. When a human loop is triggered, human review tasks are sent to the workers as specified in the flow definition.\n",
    "\n",
    "When using a custom task type, as this tutorial will show, you start a human loop using the Amazon Augmented AI Runtime API. When you call `start_human_loop()` in your custom application, a task is sent to human reviewers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup\n",
    "This notebook is developed and tested in a SageMaker Notebook Instance with a `ml.t2.medium` instance with SageMaker Python SDK v2. It is recommended to execute the notebook in the same environment for best experience.\n",
    "### Install Latest SDK"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -U sagemaker==2.23.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sagemaker\n",
    "from pkg_resources import parse_version\n",
    "\n",
    "assert parse_version(sagemaker.__version__) >= parse_version('2'), \\\n",
    "    '''This notebook is only compatible with sagemaker python SDK >= 2. \n",
    "Current version is %s. Please make sure you upgrade the library.''' % sagemaker.__version__\n",
    "\n",
    "print('SageMaker python SDK version: %s' % sagemaker.__version__)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We need to set up the following data:\n",
    "* `region` - Region to call A2I.\n",
    "* `BUCKET` - A S3 bucket accessible by the given role\n",
    "    * Used to store the sample images & output results\n",
    "    * Must be within the same region A2I is called from\n",
    "* `role` - The IAM role used as part of StartHumanLoop. By default, this notebook will use the execution role\n",
    "* `workteam` - Group of people to send the work to"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import boto3 \n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "region = my_session.region_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r endpoint_name "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Role and Permissions\n",
    "\n",
    "The AWS IAM Role used to execute the notebook needs to have the following permissions:\n",
    "\n",
    "* SagemakerFullAccess\n",
    "* AmazonSageMakerMechanicalTurkAccess (if using MechanicalTurk as your Workforce)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker import get_execution_role\n",
    "import sagemaker\n",
    "\n",
    "# Setting Role to the default SageMaker Execution Role\n",
    "role = get_execution_role()\n",
    "display(role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import boto3\n",
    "import botocore\n",
    "\n",
    "sess = sagemaker.Session()\n",
    "BUCKET = sess.default_bucket()\n",
    "TRAIN_PATH = f's3://{BUCKET}/tomofun'\n",
    "OUTPUT_PATH = f's3://{BUCKET}/a2i-results'\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Setup Bucket and Paths\n",
    "\n",
    "**Important**: The bucket you specify for `BUCKET` must have CORS enabled. You can enable CORS by adding a policy similar to the following to your Amazon S3 bucket. To learn how to add CORS to an S3 bucket, see [CORS Permission Requirement](https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html#a2i-cors-update) in the Amazon A2I documentation. \n",
    "\n",
    "\n",
    "```\n",
    "[{\n",
    "   \"AllowedHeaders\": [],\n",
    "   \"AllowedMethods\": [\"GET\"],\n",
    "   \"AllowedOrigins\": [\"*\"],\n",
    "   \"ExposeHeaders\": []\n",
    "}]\n",
    "```\n",
    "\n",
    "If you do not add a CORS configuration to the S3 buckets that contains your image input data, human review tasks for those input data objects will fail. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "cors_configuration = {\n",
    "    'CORSRules': [{\n",
    "       \"AllowedHeaders\": [],\n",
    "       \"AllowedMethods\": [\"GET\"],\n",
    "       \"AllowedOrigins\": [\"*\"],\n",
    "       \"ExposeHeaders\": []\n",
    "    }]\n",
    "}\n",
    "\n",
    "# Set the CORS configuration\n",
    "s3 = boto3.client('s3')\n",
    "s3.put_bucket_cors(Bucket=BUCKET,\n",
    "                   CORSConfiguration=cors_configuration)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Audio Classification with Amazon SageMaker\n",
    "\n",
    "To demonstrate A2I with Amazon SageMaker hosted endpoint, we will take a trained audio classification model from a S3 bucket and host it on the SageMaker endpoint for real-time prediction. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Load the model and create an endpoint\n",
    "The next cell will setup an endpoint from a trained model. It will take about 3 minutes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import boto3 \n",
    "\n",
    "my_session = boto3.session.Session()\n",
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()[\"Account\"]\n",
    "algorithm_name = \"vgg16-audio\"\n",
    "image_uri=f\"{account_id}.dkr.ecr.{region}.amazonaws.com/{algorithm_name}\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Helper functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.patches as patches    \n",
    "import matplotlib.image as mpimg\n",
    "import random\n",
    "import numpy as np\n",
    "import json\n",
    "\n",
    "runtime_client = boto3.client('runtime.sagemaker')\n",
    "\n",
    "\n",
    "    \n",
    "def load_and_predict(file_name):\n",
    "    \"\"\"\n",
    "    load an audio file, make audio classification to an predictor\n",
    "    Parameters:\n",
    "    ----------\n",
    "    file_name : str\n",
    "        image file location, in str format\n",
    "    predictor : sagemaker.predictor.RealTimePredictor\n",
    "        a predictor loaded from hosted endpoint\n",
    "    threshold : float\n",
    "        score threshold for bounding box display\n",
    "    \"\"\"\n",
    "    with open(file_name, 'rb') as image:\n",
    "        f = image.read()\n",
    "        b = bytearray(f)\n",
    "    response = runtime_client.invoke_endpoint(EndpointName=endpoint_name, \n",
    "                                   ContentType='application/octet-stream', \n",
    "                                   Body=b)\n",
    "    results = response['Body'].read().decode('utf-8')\n",
    "\n",
    "    print(results)\n",
    "\n",
    "    detections = json.loads(results)\n",
    "    return results, detections"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories = [\"Barking\", \"Howling\", \"Crying\", \"COSmoke\",\"GlassBreaking\",\"Other\"]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sample Data\n",
    "Let's take a look how the audio classification model looks like using some audio clips on our hands. The predicted class and the prediction probability is presented."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!mkdir audios \n",
    "!cp ../01-byoc/input/data/competition/train/train_00001.wav audios \n",
    "!cp ../01-byoc/input/data/competition/train/train_00010.wav audios \n",
    "!cp ../01-byoc/input/data/competition/train/train_00021.wav audios "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_audios = ['audios/train_00001.wav', # motorcycle\n",
    "               'audios/train_00010.wav', # bicycle\n",
    "               'audios/train_00021.wav'] # sofa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import IPython.display as ipd\n",
    "ipd.Audio(test_audios[0], autoplay=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "for audio in test_audios: \n",
    "    results, detections = load_and_predict(audio)\n",
    "    print(detections) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Probability of 0.465 is considered quite low in modern computer vision and there is a mislabeling. This is due to the fact that the SSD model was under-trained for demonstration purposes in the [training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb). However this under-trained model serves as a perfect example of brining human reviewers when a model is unable to make a high confidence prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Creating human review Workteam or Workforce"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A workforce is the group of workers that you have selected to label your dataset. You can choose either the Amazon Mechanical Turk workforce, a vendor-managed workforce, or you can create your own private workforce for human reviews. Whichever workforce type you choose, Amazon Augmented AI takes care of sending tasks to workers. \n",
    "\n",
    "When you use a private workforce, you also create work teams, a group of workers from your workforce that are assigned to Amazon Augmented AI human review tasks. You can have multiple work teams and can assign one or more work teams to each job."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To create your Workteam, visit the instructions here: https://docs.aws.amazon.com/sagemaker/latest/dg/sms-workforce-management.html\n",
    "\n",
    "After you have created your workteam, replace YOUR_WORKTEAM_ARN below"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "my_session = boto3.session.Session()\n",
    "my_region = my_session.region_name\n",
    "client = boto3.client(\"sts\")\n",
    "account_id = client.get_caller_identity()[\"Account\"]\n",
    "\n",
    "WORKTEAM_ARN = \"arn:aws:sagemaker:{}:{}:workteam/private-crowd/seal-squad\".format(my_region, account_id)\n",
    "WORKTEAM_ARN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Visit: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-permissions-security.html to add the necessary permissions to your role"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Client Setup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here we are going to setup the rest of our clients. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import uuid\n",
    "import time \n",
    "\n",
    "timestamp = time.strftime(\"%Y-%m-%d-%H-%M-%S\", time.gmtime())\n",
    "# Amazon SageMaker client\n",
    "sagemaker_client = boto3.client('sagemaker', region)\n",
    "s3_client = boto3.client('s3')\n",
    "\n",
    "# Amazon Augment AI (A2I) client\n",
    "a2i = boto3.client('sagemaker-a2i-runtime')\n",
    "\n",
    "# Amazon S3 client \n",
    "s3 = boto3.client('s3', region)\n",
    "\n",
    "# Flow definition name - this value is unique per account and region. You can also provide your own value here.\n",
    "flowDefinitionName = 'fd-sagemaker-audio-classification-demo-' + timestamp\n",
    "\n",
    "# Task UI name - this value is unique per account and region. You can also provide your own value here.\n",
    "taskUIName = 'ui-sagemaker-audio-classification-demo-' + timestamp"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Create Control Plane Resources"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create Human Task UI\n",
    "\n",
    "Create a human task UI resource, giving a UI template in liquid html. This template will be rendered to the human workers whenever human loop is required.\n",
    "\n",
    "For over 70 pre built UIs, check: https://github.com/aws-samples/amazon-a2i-sample-task-uis.\n",
    "\n",
    "We will be taking an [audio classification UI](https://github.com/aws-samples/amazon-sagemaker-ground-truth-task-uis/blob/master/audio/audio-classification.liquid.html) and filling in the object categories in the `labels` variable in the template."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# task.input.taskObject\n",
    "\n",
    "template = r\"\"\"\n",
    "<script src=\"https://assets.crowd.aws/crowd-html-elements.js\"></script>\n",
    "\n",
    "<crowd-form>\n",
    "    <crowd-classifier\n",
    "      name=\"sentiment\"\n",
    "      categories=\"['Barking', 'Howling', 'Crying', 'COSmoke','GlassBreaking','Other']\"\n",
    "      header=\"What class does this audio represent?\"\n",
    "    >\n",
    "      <classification-target>\n",
    "          <audio controls>\n",
    "              <source src=\"{{ task.input.taskObject | grant_read_access }}\" type=\"audio/wav\">\n",
    "              Your browser does not support the audio element.\n",
    "          </audio>\n",
    "      </classification-target>\n",
    "      \n",
    "      <full-instructions header=\"Audio Classification Analysis Instructions\">\n",
    "        <p><strong>Barking</strong>Barking </p>\n",
    "        <p><strong>Howling</strong>Howling</p>\n",
    "        <p><strong>Crying</strong>Crying</p>\n",
    "        <p><strong>COSmoke</strong>COSmoke</p>\n",
    "        <p><strong>GlassBreaking</strong>GlassBreaking</p>\n",
    "        <p><strong>Other</strong>Other</p>\n",
    "      </full-instructions>\n",
    "\n",
    "      <short-instructions>\n",
    "        <p>Choose the primary sentiment that is expressed by the audio.</p>\n",
    "      </short-instructions>\n",
    "    </crowd-classifier>\n",
    "</crowd-form>\n",
    "\"\"\"\n",
    "\n",
    "def create_task_ui():\n",
    "    '''\n",
    "    Creates a Human Task UI resource.\n",
    "\n",
    "    Returns:\n",
    "    struct: HumanTaskUiArn\n",
    "    '''\n",
    "    response = sagemaker_client.create_human_task_ui(\n",
    "        HumanTaskUiName=taskUIName,\n",
    "        UiTemplate={'Content': template})\n",
    "    return response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create task UI\n",
    "humanTaskUiResponse = create_task_ui()\n",
    "humanTaskUiArn = humanTaskUiResponse['HumanTaskUiArn']\n",
    "print(humanTaskUiArn)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create the Flow Definition"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this section, we're going to create a flow definition definition. Flow Definitions allow us to specify:\n",
    "\n",
    "* The workforce that your tasks will be sent to.\n",
    "* The instructions that your workforce will receive. This is called a worker task template.\n",
    "* The configuration of your worker tasks, including the number of workers that receive a task and time limits to complete tasks.\n",
    "* Where your output data will be stored.\n",
    "\n",
    "This demo is going to use the API, but you can optionally create this workflow definition in the console as well. \n",
    "\n",
    "For more details and instructions, see: https://docs.aws.amazon.com/sagemaker/latest/dg/a2i-create-flow-definition.html."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "create_workflow_definition_response = sagemaker_client.create_flow_definition(\n",
    "        FlowDefinitionName= flowDefinitionName,\n",
    "        RoleArn= role,\n",
    "        HumanLoopConfig= {\n",
    "            \"WorkteamArn\": WORKTEAM_ARN,\n",
    "            \"HumanTaskUiArn\": humanTaskUiArn,\n",
    "            \"TaskCount\": 1,\n",
    "            \"TaskDescription\": \"Classify the audio category.\",\n",
    "            \"TaskTitle\": \"Audio Classification\"\n",
    "        },\n",
    "        OutputConfig={\n",
    "            \"S3OutputPath\" : OUTPUT_PATH\n",
    "        }\n",
    "    )\n",
    "flowDefinitionArn = create_workflow_definition_response['FlowDefinitionArn'] # let's save this ARN for future use"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Describe flow definition - status should be active\n",
    "for x in range(60):\n",
    "    describeFlowDefinitionResponse = sagemaker_client.describe_flow_definition(FlowDefinitionName=flowDefinitionName)\n",
    "    print(describeFlowDefinitionResponse['FlowDefinitionStatus'])\n",
    "    if (describeFlowDefinitionResponse['FlowDefinitionStatus'] == 'Active'):\n",
    "        print(\"Flow Definition is active\")\n",
    "        break\n",
    "    time.sleep(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create SQS queue and pass a2i task status change event to the queue\n",
    "<a id=\"sqs_a2i\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqs = boto3.resource('sqs')\n",
    "queue_name = 'a2itasks'\n",
    "queue_arn = \"arn:aws:sqs:{}:{}:{}\".format(region, account_id, queue_name)\n",
    "\n",
    "policy = '''{\n",
    "            \"Version\": \"2012-10-17\",\n",
    "            \"Id\": \"MyQueuePolicy\",\n",
    "            \"Statement\": [{                     \n",
    "                    \"Effect\": \"Allow\",\n",
    "                    \"Principal\": {\n",
    "                            \"Service\": [\"events.amazonaws.com\",\n",
    "                            \"sqs.amazonaws.com\"]\n",
    "                    },\n",
    "                    \"Action\": \"sqs:SendMessage\"\n",
    "            }]}'''\n",
    "policy_obj = json.loads(policy)\n",
    "policy_obj['Statement'][0]['Resource'] = queue_arn\n",
    "policy = json.dumps(policy_obj)\n",
    "\n",
    "queue = sqs.create_queue(QueueName=queue_name, Attributes={'DelaySeconds': '0',\n",
    "                                                                'Policy': policy})\n",
    "print(queue.url)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(queue)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqs_client = boto3.client('sqs')\n",
    "\n",
    "sqs_client.add_permission(\n",
    "    QueueUrl=queue.url,\n",
    "    Label=\"a2i\",\n",
    "    AWSAccountIds=[\n",
    "        account_id,\n",
    "    ],\n",
    "    Actions=[\n",
    "        'SendMessage',\n",
    "    ]\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "iam = boto3.client(\"iam\")\n",
    "\n",
    "role_name = \"AmazonSageMaker-SageMakerExecutionRole\"\n",
    "assume_role_policy_document = {\n",
    "    \"Version\": \"2012-10-17\",\n",
    "    \"Statement\": [\n",
    "        {\n",
    "          \"Effect\": \"Allow\",\n",
    "          \"Principal\": {\n",
    "            \"Service\": [\"sagemaker.amazonaws.com\", \"events.amazonaws.com\"]\n",
    "          },\n",
    "          \"Action\": \"sts:AssumeRole\"\n",
    "        }\n",
    "    ]\n",
    "}\n",
    "\n",
    "create_role_response = iam.create_role(\n",
    "    RoleName = role_name,\n",
    "    AssumeRolePolicyDocument = json.dumps(assume_role_policy_document)\n",
    ")\n",
    "\n",
    "\n",
    "# Now add S3 support\n",
    "iam.attach_role_policy(\n",
    "    PolicyArn='arn:aws:iam::aws:policy/AmazonS3FullAccess',\n",
    "    RoleName=role_name\n",
    ")\n",
    "time.sleep(60) # wait for a minute to allow IAM role policy attachment to propagate\n",
    "\n",
    "sm_role_arn = create_role_response[\"Role\"][\"Arn\"]\n",
    "print(sm_role_arn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash  -s \"$sm_role_arn\" \"$my_region\" \n",
    "aws events put-rule --name \"A2IHumanLoopStatusChanges\" \\\n",
    "    --event-pattern \"{\\\"source\\\":[\\\"aws.sagemaker\\\"],\\\"detail-type\\\":[\\\"SageMaker A2I HumanLoop Status Change\\\"]}\" \\\n",
    "    --role-arn \"$1\" \\\n",
    "    --region $2 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!sed \"s/<account_id>/$account_id/g\" targets-template.json > targets-tmp.json \n",
    "!sed \"s/<region>/$my_region/g\" targets-tmp.json  > targets.json "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws events put-targets --rule A2IHumanLoopStatusChanges \\\n",
    "--targets file://$PWD/targets.json"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Have newly created SQS queue as a target of the rule we just defined "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Starting Human Loops"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have setup our Flow Definition, we are ready to call our object detection endpoint on SageMaker and start our human loops. In this tutorial, we are interested in starting a HumanLoop only if the highest prediction probability score returned by our model for objects detected is less than 50%. \n",
    "\n",
    "So, with a bit of logic, we can check the response for each call to the SageMaker endpoint using `load_and_predict` helper function, and if the highest score is less than 50%, we will kick off a HumanLoop to engage our workforce for a human review. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the sample images to s3 bucket for a2i UI to display\n",
    "!aws s3 sync ./audios/ s3://{BUCKET}/audios/\n",
    "    \n",
    "human_loops_started = []\n",
    "SCORE_THRESHOLD = .50\n",
    "import json\n",
    "for fname in test_audios:\n",
    "    # Call SageMaker endpoint and not display any object detected with probability lower than 0.4.\n",
    "\n",
    "    # Sort by prediction score so that the first item has the highest probability\n",
    "    result, detections = load_and_predict(audio)\n",
    "    max_p = max(detections['probability']) \n",
    "\n",
    "    # Our condition for triggering a human review\n",
    "    if max_p < SCORE_THRESHOLD:\n",
    "        s3_fname='s3://%s/%s' % (BUCKET, fname)\n",
    "        print(s3_fname)\n",
    "        humanLoopName = str(uuid.uuid4())\n",
    "        inputContent = {\n",
    "            \"initialValue\": max_p,\n",
    "            \"taskObject\": s3_fname # the s3 object will be passed to the worker task UI to render\n",
    "        }\n",
    "        # start an a2i human review loop with an input\n",
    "        start_loop_response = a2i.start_human_loop(\n",
    "            HumanLoopName=humanLoopName,\n",
    "            FlowDefinitionArn=flowDefinitionArn,\n",
    "            HumanLoopInput={\n",
    "                \"InputContent\": json.dumps(inputContent)\n",
    "            }\n",
    "        )\n",
    "        print(start_loop_response)\n",
    "        human_loops_started.append(humanLoopName)\n",
    "        print(f'Object detection Confidence Score of %s is less than the threshold of %.2f' % (max_p, SCORE_THRESHOLD))\n",
    "        print(f'Starting human loop with name: {humanLoopName}  \\n')\n",
    "    else:\n",
    "        print(f'Object detection Confidence Score of %s is above than the threshold of %.2f' % (max_p, SCORE_THRESHOLD))\n",
    "        print('No human loop created. \\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Check Status of Human Loop"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(resp) \n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Wait For Workers to Complete Task\n",
    "Since we are using private workteam, we should go to the labling UI to perform the inspection ourselves."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "workteamName = WORKTEAM_ARN[WORKTEAM_ARN.rfind('/') + 1:]\n",
    "print(\"Navigate to the private worker portal and do the tasks. Make sure you've invited yourself to your workteam!\")\n",
    "print('https://' + sagemaker_client.describe_workteam(WorkteamName=workteamName)['Workteam']['SubDomain'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "completed_human_loops = []\n",
    "for human_loop_name in human_loops_started:\n",
    "    resp = a2i.describe_human_loop(HumanLoopName=human_loop_name)\n",
    "    print(resp) \n",
    "    print(f'HumanLoop Name: {human_loop_name}')\n",
    "    print(f'HumanLoop Status: {resp[\"HumanLoopStatus\"]}')\n",
    "    print(f'HumanLoop Output Destination: {resp[\"HumanLoopOutput\"]}')\n",
    "    print('\\n')\n",
    "    \n",
    "    if resp[\"HumanLoopStatus\"] == \"Completed\":\n",
    "        completed_human_loops.append(resp)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Collect data from a2i to build the training data for the next round "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "queue.url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "sqs = boto3.client('sqs')\n",
    "completed_human_loops = []\n",
    "while True: \n",
    "    response = sqs.receive_message(\n",
    "        QueueUrl=queue.url,\n",
    "\n",
    "        MaxNumberOfMessages=10,\n",
    "        MessageAttributeNames=[\n",
    "            'All'\n",
    "        ],\n",
    "        VisibilityTimeout=10,\n",
    "        WaitTimeSeconds=0\n",
    "    )\n",
    "    if 'Messages' not in response: \n",
    "        break \n",
    "    messages = response['Messages']\n",
    "\n",
    "    for m in messages: \n",
    "        task = json.loads(m['Body'])['detail']\n",
    "        name = task['humanLoopName']\n",
    "        output_s3 = task['humanLoopOutput']['outputS3Uri']\n",
    "        completed_human_loops.append((name, output_s3))\n",
    "        receipt_handle = m['ReceiptHandle']\n",
    "\n",
    "        # Delete received message from queue\n",
    "        sqs.delete_message(\n",
    "            QueueUrl=queue.url,\n",
    "            ReceiptHandle=receipt_handle\n",
    "        )\n",
    "    \n",
    "print(completed_human_loops)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### View Task Results  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Once work is completed, Amazon A2I stores results in your S3 bucket and sends a Cloudwatch event. Your results should be available in the S3 OUTPUT_PATH when all work is completed. Note that the human answer, the label and the bounding box, is returned and saved in the json file."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import pprint\n",
    "\n",
    "pp = pprint.PrettyPrinter(indent=4)\n",
    "\n",
    "for name, s3_output_path in completed_human_loops:\n",
    "    splitted_string = re.split('s3://' +  BUCKET + '/',s3_output_path)\n",
    "    output_bucket_key = splitted_string[1]\n",
    "\n",
    "    response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "    content = response[\"Body\"].read()\n",
    "    json_output = json.loads(content)\n",
    "    pp.pprint(json_output)\n",
    "    print('\\n')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Incremental training with SageMaker\n",
    "Now that we have used the model to generate prediction on some random out-of-sample images and got unsatisfactory prediction (low probability). We also demonstrated how to use Amazon Augmented AI to review and label the image based on custom criteria. Next step in a typical machine learning life cycle is to include these cases with which the model has trouble in the next batch of training data for retraining purposes so that the model can now learn from a set of new training data to improve the model. In machine learning we call it [incremental training](https://docs.aws.amazon.com/sagemaker/latest/dg/incremental-training.html).\n",
    "\n",
    "Now we can obtain the result of a2i tasks and formulated the information into the format of our training data - \n",
    "* the meta data in csv file format\n",
    "```\n",
    "Filename,Label,Remark\n",
    "train_00021,1,Howling\n",
    "```\n",
    "* and associating audio files on s3 "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories_dict = {j: i for i, j in enumerate(object_categories)}\n",
    "\n",
    "def convert_a2i_to_augmented_manifest(a2i_output):\n",
    "    label = a2i_output['humanAnswers'][0]['answerContent']['sentiment']['label']\n",
    "    s3_path = a2i_output['inputContent']['taskObject']\n",
    "    filename = s3_path.split('/')[-1][:-4]\n",
    "    label_id = str(object_categories_dict[label]) \n",
    "    return '{},{},{}'.format(filename, label_id, label), s3_path\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "object_categories_dict"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function will take an A2I output json and result in a json object that is compatible to how Amazon SageMaker Ground Truth outputs the result and how SageMaker built-in object detection algorithm expects from the input. In order to create a cohort of training images from all the images re-labeled by human reviewers in A2I console. You can loop through all the A2I output, convert the json file, and concatenate them into a JSON Lines file, with each line represents results of one image. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "s3_paths=[]\n",
    "with open('augmented.manifest', 'w') as outfile:\n",
    "    outfile.write(\"Filename,Label,Remark\\n\")\n",
    "    # convert the a2i json to augmented manifest for each human loop output\n",
    "    for name, s3_output_path in completed_human_loops:\n",
    "        splitted_string = re.split('s3://' +  BUCKET + '/', s3_output_path)\n",
    "        output_bucket_key = splitted_string[1]\n",
    "\n",
    "        response = s3.get_object(Bucket=BUCKET, Key=output_bucket_key)\n",
    "        content = response[\"Body\"].read()\n",
    "        json_output = json.loads(content)\n",
    "        print(json_output)\n",
    "        # convert using the function\n",
    "        augmented_manifest, s3_path = convert_a2i_to_augmented_manifest(json_output)\n",
    "        s3_paths.append(s3_path)\n",
    "        outfile.write(augmented_manifest)\n",
    "        outfile.write('\\n')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take a look at how Json Lines looks like\n",
    "!head -n2 augmented.manifest"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# upload the manifest file to S3\n",
    "import time;\n",
    "ts = time.time()\n",
    "\n",
    "\n",
    "train_path = f\"{TRAIN_PATH}/{ts}/competition\"\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws s3 cp augmented.manifest {train_path}/meta_train.csv\n",
    "for s3_path in s3_paths: \n",
    "    filename = s3_path.split('/')[-1]\n",
    "    !aws s3 cp {s3_path} {train_path}/train/{filename} "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Similar to training with Ground Truth output augmented manifest file outlined in this [blog](https://aws.amazon.com/blogs/machine-learning/easily-train-models-using-datasets-labeled-by-amazon-sagemaker-ground-truth/), once we have collected enough data points, we can construct a new `Estimator` for incremental training. \n",
    "\n",
    "For incremental training, the choice of hyperparameters becomes critical. Since we are continue the learning and optimization from the last model, an appropriate starting `learning_rate`, for example, would again need to be determined. But as a rule of thumb, even with the introduction of new, unseen data, we should start out the incremental training with a smaller `learning_rate` and different learning rate schedule (`lr_scheduler_factor` and `lr_scheduler_step`) than that of the previous training job as the optimization has previously reached to a more stable state with reduced learning rate. We should see a similar mAP performance on the original validation dataset in the first epoch in the incremental training. \n",
    "\n",
    "We here will be using the hyperparameters exactly the same as how the first model was trained in the [training notebook](https://github.com/awslabs/amazon-sagemaker-examples/blob/master/introduction_to_amazon_algorithms/object_detection_pascalvoc_coco/object_detection_recordio_format.ipynb), with the following exceptions\n",
    "\n",
    "- smaller learning rate (`learning_rate` was 0.001, now 0.0001)\n",
    "- using the weights from the trained model instead of pre-trained weights that comes with the algorithm (`use_pretrained_model=0`).\n",
    "\n",
    "Note that the following working code snippet is meant to demonstrate how to set up the A2I output for training in SageMaker with object detection algorithm. Incremental training with merely 1 or 2 new samples and untuned hyperparameters, would not yield a meaning model, if not experiencing [catastrophic forgetting](https://en.wikipedia.org/wiki/Catastrophic_interference).\n",
    "\n",
    "*The next cell would take about 5 minutes.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r model_s3_path"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path definition\n",
    "s3_train_data = train_path\n",
    "# Reusing the training data for validation here for demonstration purposes\n",
    "# but in practice you should provide a set of data that you want to validate the training against\n",
    "s3_validation_data = train_path \n",
    "s3_output_location = f'{OUTPUT_PATH}/incremental-training'\n",
    "\n",
    "# num_training_samples = len(output)\n",
    "num_training_samples = 3 \n",
    "\n",
    "# Create a model object set to using \"Pipe\" mode because we are inputing augmented manifest files.\n",
    "new_od_model = sagemaker.estimator.Estimator(image_uri, # same object detection image that we used for model hosting  \n",
    "                                             role, \n",
    "                                             instance_count=1, \n",
    "                                             instance_type='ml.p3.2xlarge', \n",
    "                                             volume_size = 50, \n",
    "                                             max_run = 360000, \n",
    "                                             input_mode = 'File',\n",
    "                                             output_path=s3_output_location, \n",
    "                                             sagemaker_session=sess) \n",
    "\n",
    "# same set of hyperparameters from the original training job\n",
    "new_od_model.set_hyperparameters(batch_size = 1)\n",
    "\n",
    "# setting the input data\n",
    "train_data = sagemaker.inputs.TrainingInput(s3_train_data)\n",
    "validation_data = sagemaker.inputs.TrainingInput(s3_validation_data)\n",
    "\n",
    "# Use the output model from the original training job.  \n",
    "model_data = sagemaker.inputs.TrainingInput(model_s3_path)\n",
    "\n",
    "data_channels = {'competition': train_data, \n",
    "                 'model': model_data}\n",
    "                 \n",
    "new_od_model.fit(inputs=data_channels, logs=True, wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "After training, you would get a new model in the `s3_output_location`, you can deploy it to a new endpoint or modify an endpoint without taking models that are already deployed into production out of service. For example, you can add new model variants, update the ML Compute instance configurations of existing model variants, or change the distribution of traffic among model variants. To modify an endpoint, you provide a new endpoint configuration. Amazon SageMaker implements the changes without any downtime. For more information, see [UpdateEndpoint](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpoint.html) and [UpdateEndpointWeightsAndCapacities](https://docs.aws.amazon.com/sagemaker/latest/APIReference/API_UpdateEndpointWeightsAndCapacities.html). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_od_model.model_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "incremented_model = sagemaker.model.Model(image_uri, \n",
    "                              model_data = new_od_model.model_data,\n",
    "                              role = role,\n",
    "                              predictor_cls = sagemaker.predictor.Predictor,\n",
    "                              sagemaker_session = sess)\n",
    "\n",
    "new_detector =  sagemaker.predictor.Predictor(endpoint_name = endpoint_name) \n",
    "new_detector.update_endpoint(model_name=incremented_model.name, initial_instance_count = 1,\n",
    "                               instance_type = 'ml.p2.xlarge', wait=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create a Lambda function pass samples with low confidence to a2i \n",
    "<a id=\"lambda\"></a>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s \"$BUCKET\" \n",
    "cd invoke_endpoint_a2i \n",
    "zip -r invoke_endpoint_a2i.zip  .\n",
    "aws s3 cp invoke_endpoint_a2i.zip s3://$1/lambda/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store -r lambda_role_arn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "cwd = os.getcwd()\n",
    "!aws lambda create-function --function-name invoke_endpoint_a2i --zip-file fileb://$cwd/invoke_endpoint_a2i/invoke_endpoint_a2i.zip  --handler lambda_function.lambda_handler --runtime python3.7 --role $lambda_role_arn "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Configure lambda function - invoke_image_object_detection \n",
    "* you can also do it by command line - \n",
    "```\n",
    "aws lambda update-function-configuration --function-name invoke_image_object_detection \\\n",
    "    --environment \"Variables={BUCKET=my-bucket,KEY=file.txt}\"\n",
    "```    \n",
    "![configure environment variable](../03-lambda-api/content_image/setup_env_vars_for_lambda2.png)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "bucket_key = \"a2i-demo\"\n",
    "variables = f\"A2IFLOW_DEF={flowDefinitionArn},BUCKET={BUCKET},ENDPOINT_NAME={endpoint_name},KEY={bucket_key}\"\n",
    "env = \"Variables={\"+variables+\"}\"\n",
    "\n",
    "!aws lambda update-function-configuration --function-name invoke_endpoint_a2i --environment \"$env\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!aws lambda add-permission \\\n",
    "    --function-name invoke_endpoint_a2i \\\n",
    "    --action lambda:InvokeFunction \\\n",
    "    --statement-id apigateway \\\n",
    "    --principal apigateway.amazonaws.com  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Integrate the Lambda with API Gateway \n",
    "* reference to the previous notebook "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced material - use sagemaker pipeline to manege the training / deployment process "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.parameters import (\n",
    "    ParameterInteger,\n",
    "    ParameterString,\n",
    ")\n",
    "\n",
    "\n",
    "train_data = ParameterString(\n",
    "    name=\"TrainData\",\n",
    "    default_value=s3_train_data,\n",
    ")\n",
    "validation_data = ParameterString(\n",
    "    name=\"ValidationData\",\n",
    "    default_value=s3_validation_data,\n",
    ")\n",
    "model_data = ParameterString(\n",
    "    name=\"ModelData\",\n",
    "    default_value=model_s3_path,\n",
    ")\n",
    "model_approval_status = ParameterString(\n",
    "    name=\"ModelApprovalStatus\",\n",
    "    default_value=\"Approved\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.steps import TrainingStep\n",
    "\n",
    "\n",
    "step_train = TrainingStep(\n",
    "    name=\"AudioClassificationTraining\",\n",
    "    estimator=new_od_model,\n",
    "    inputs={\n",
    "        \"competition\": sagemaker.inputs.TrainingInput(train_data, \n",
    "                                            distribution='FullyReplicated'), \n",
    "        \"validation\":sagemaker.inputs.TrainingInput(validation_data, \n",
    "                                                 distribution='FullyReplicated'), \n",
    "        \"model\":sagemaker.inputs.TrainingInput(model_data, \n",
    "                                            distribution='FullyReplicated')\n",
    "    },\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time \n",
    "from sagemaker.workflow.step_collections import CreateModelStep\n",
    "model_name='audio-vgg16-'+str(int(time.time())) \n",
    "\n",
    "model = sagemaker.model.Model(\n",
    "    name=model_name,\n",
    "    image_uri=step_train.properties.AlgorithmSpecification.TrainingImage,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    sagemaker_session=sess,\n",
    "    role=role\n",
    ")\n",
    "\n",
    "inputs = sagemaker.inputs.CreateModelInput(\n",
    "    instance_type=\"ml.m4.xlarge\"\n",
    ")\n",
    "\n",
    "create_model_step = CreateModelStep(\n",
    "    name=\"ModelPreDeployment\",\n",
    "    model=model,\n",
    "    inputs=inputs\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.workflow.step_collections import RegisterModel\n",
    "model_package_group_name = f\"AudioClassificationGroupModel\" \n",
    "step_register = RegisterModel(\n",
    "    name=\"AudioClassificationModel\",\n",
    "    estimator=new_od_model,\n",
    "    model_data=step_train.properties.ModelArtifacts.S3ModelArtifacts,\n",
    "    content_types=[\"application/octet-stream\"],\n",
    "    response_types=[\"application/json\"],\n",
    "    inference_instances=[\"ml.t2.medium\", \"ml.m5.xlarge\"],\n",
    "    transform_instances=[\"ml.m5.xlarge\"],\n",
    "    model_package_group_name=model_package_group_name,\n",
    "    approval_status=model_approval_status,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sagemaker.sklearn.processing import SKLearnProcessor\n",
    "from sagemaker.workflow.steps import ProcessingStep\n",
    "\n",
    "\n",
    "\n",
    "deploy_model_processor = SKLearnProcessor(\n",
    "    framework_version='0.23-1',\n",
    "    role=role,\n",
    "    instance_type=\"ml.m5.large\",\n",
    "    instance_count=1,\n",
    "    sagemaker_session=sess)\n",
    "\n",
    "deploy_step = ProcessingStep(\n",
    "    name='DeployModel',\n",
    "    processor=deploy_model_processor,\n",
    "    job_arguments=[\n",
    "        \"--model-name\", create_model_step.properties.ModelName,\n",
    "        \"--endpoint-name\", endpoint_name, \n",
    "        \"--region\", region],\n",
    "    code=\"./deploy_model.py\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "endpoint_name"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline_name=\"AudioClassification\"\n",
    "from sagemaker.workflow.pipeline import Pipeline\n",
    "pipeline = Pipeline(\n",
    "    name=pipeline_name,\n",
    "    parameters=[\n",
    "        train_data, validation_data, model_data, model_approval_status \n",
    "    ],\n",
    "    steps=[ step_train, step_register, create_model_step, deploy_step],\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "json.loads(pipeline.definition())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline.upsert(role_arn=role)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "execution = pipeline.start()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### More on incremental training\n",
    "It is recommended to perform a search over the hyperparameter space for your incremental training with [hyperparameter tuning](https://docs.aws.amazon.com/sagemaker/latest/dg/automatic-model-tuning.html) for an optimal set of hyperparameters, especially the ones related to learning rate: `learning_rate`, `lr_scheduler_factor` and `lr_scheduler_step` from the SageMaker object detection algorithm. We have an [example](https://github.com/aws/amazon-sagemaker-examples/blob/master/hyperparameter_tuning/image_classification_early_stopping/hpo_image_classification_early_stopping.ipynb) of running a hyperparameter tuning job using Amazon SageMaker Automatic Model Tuning feature. Please try it out!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The End, but....!\n",
    "This is the end of the example. Remember to execute the next cell to delete the endpoint otherwise it will continue to incur charges."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%store flowDefinitionArn \n",
    "%store endpoint_name\n",
    "%store model_package_group_name \n",
    "%store pipeline_name\n",
    "%store role\n",
    "%store lambda_role_arn\n",
    "#object_detector.delete_endpoint()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "conda_python3",
   "language": "python",
   "name": "conda_python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
